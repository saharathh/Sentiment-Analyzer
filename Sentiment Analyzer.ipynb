{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f901419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import joblib\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75606edd",
   "metadata": {},
   "source": [
    "## **Load Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ca441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('WiseSight Tokenisation Annotation - WiseSight (250 samples_class).csv', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a77ca2",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f80e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_removed = ~df.label.apply(lambda x: len(x.split(\"-\")) > 1)\n",
    "data_for_training = df[should_removed]\n",
    "print(\"we have %d after samples\" % len(data_for_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thai stopwords\n",
    "thai_stopwords = list(thai_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text):\n",
    "    URL_PATTERN = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    return re.sub(URL_PATTERN, 'xxurl', text)\n",
    "\n",
    "#ลบคำ เช่น ไวววววววววว ให้เป็น ไว\n",
    "#(.) means แทนตัวไหนก็ได้\n",
    "#\\1 means first group\n",
    "#{3,} means ซ้ำมากกว่า 3 ตัว\n",
    "def clean_rep(text):\n",
    "    return re.compile(r'(.)\\1{3,}', re.IGNORECASE).sub(r'\\1', text)\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    return re.sub(\"#(\\w+)\", '', text)\n",
    "    \n",
    "#ลบคำ เช่น ไวไวไวไวไว ให้เป็น ไว\n",
    "def remove_duplicate(text):\n",
    "    return re.sub(r'(.+?)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = clean_url(text)\n",
    "    text = remove_hashtag(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = clean_rep(text)\n",
    "    text = remove_duplicate(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', \"ๆ\", \"ฯ\", \"#\"))\n",
    "    text = \"\".join(word for word in text if word not in thai_stopwords)\n",
    "    \n",
    "    #tokenize\n",
    "    text = [word for word in word_tokenize(text) if word and not re.search(pattern=r\"\\s+\", string=word)]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaaf3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training['tokenize_process'] = data_for_training.raw.map(lambda x: '|'.join(clean_text(x)))\n",
    "data_for_training['wc'] = data_for_training.tokenize_process.map(lambda x: len(x.split('|')))\n",
    "data_for_training['uwc'] = data_for_training.tokenize_process.map(lambda x: len(set(x.split('|'))))\n",
    "data_for_training['text_tokens'] = data_for_training.tokenize_process.str.replace('|',' ')\n",
    "data_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15d86e",
   "metadata": {},
   "source": [
    "# **Training Sentiment Analyzer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, test, split the data\n",
    "\n",
    "X = data_for_training[['text_tokens']]\n",
    "y = data_for_training['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using CountVetorizer to convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "cvec = CountVectorizer(analyzer=lambda x:x.split(' '))\n",
    "cvec.fit_transform(X_train['text_tokens'])\n",
    "train_bow = cvec.transform(X_train['text_tokens'])\n",
    "pd.DataFrame(train_bow.toarray(), columns=cvec.get_feature_names(), index=X_train['text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the above BOW to train a LogisticRegression for a Sentiment Analyzer\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score\n",
    "\n",
    "test_bow = cvec.transform(X_test['text_tokens'])\n",
    "test_predictions = lr.predict(test_bow)\n",
    "print(classification_report(test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff02e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr , 'sentiment_model_jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831b5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
