{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f901419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#for pickle a countvectorizer with lambda function\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75606edd",
   "metadata": {},
   "source": [
    "## **Load Training Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026bcd1",
   "metadata": {},
   "source": [
    "Wisesight Dataset = 'https://docs.google.com/spreadsheets/d/1F_qT33T2iy0tKbflnVC8Ma-EoWEHimV3NmNRgLjN00o/edit#gid=1302375309'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ca441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('WiseSight Tokenisation Annotation - WiseSight (250 samples_class).csv', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a77ca2",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f80e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_removed = ~df.label.apply(lambda x: len(x.split(\"-\")) > 1)\n",
    "data_for_training = df[should_removed]\n",
    "print(\"we have %d after samples\" % len(data_for_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thai stopwords\n",
    "thai_stopwords = list(thai_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text):\n",
    "    URL_PATTERN = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    return re.sub(URL_PATTERN, 'xxurl', text)\n",
    "\n",
    "#ลบคำ เช่น ไวววววววววว ให้เป็น ไว\n",
    "#(.) means แทนตัวไหนก็ได้\n",
    "#\\1 means first group\n",
    "#{3,} means ซ้ำมากกว่า 3 ตัว\n",
    "def clean_rep(text):\n",
    "    return re.compile(r'(.)\\1{3,}', re.IGNORECASE).sub(r'\\1', text)\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    return re.sub(\"#(\\w+)\", '', text)\n",
    "    \n",
    "#ลบคำ เช่น ไวไวไวไวไว ให้เป็น ไว\n",
    "def remove_duplicate(text):\n",
    "    return re.sub(r'(.+?)\\1+', r'\\1', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = clean_url(text)\n",
    "    text = remove_hashtag(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = clean_rep(text)\n",
    "    text = remove_duplicate(text)\n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    #tokenize\n",
    "    text = word_tokenize(text)\n",
    "    text = \" \".join(word for word in text)\n",
    "    text = \" \".join(word for word in text.split() if word.lower not in thai_stopwords)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaaf3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training['text_tokens'] = data_for_training['raw'].apply(clean_text)\n",
    "data_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15d86e",
   "metadata": {},
   "source": [
    "# **Training Count Vectorizor & Sentiment Analyzer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, test, split the data\n",
    "\n",
    "X = data_for_training[['text_tokens']]\n",
    "y = data_for_training['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training CountVetorizer to use for converting a collection of text documents to a matrix of token counts\n",
    "\n",
    "cvec = CountVectorizer(analyzer=lambda x:x.split(' '))\n",
    "cvec.fit_transform(X_train['text_tokens'])\n",
    "cvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = cvec.transform(X_train['text_tokens'])\n",
    "pd.DataFrame(train_bow.toarray(), columns=cvec.get_feature_names(), index=X_train['text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the above BOW to train a LogisticRegression for a Sentiment Analyzer\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Score\n",
    "\n",
    "test_bow = cvec.transform(X_test['text_tokens'])\n",
    "test_predictions = lr.predict(test_bow)\n",
    "print(classification_report(test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_model_pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(lr, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3715e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('countvectorizer_dill', 'wb') as dill_file:\n",
    "    dill.dump(cvec, dill_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7059303",
   "metadata": {},
   "source": [
    "# **Testing CountVectorizor & Sentiment Analyzer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_model_pkl', 'rb') as s:\n",
    "    model = pickle.load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df753e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('countvectorizer_dill', 'rb') as c:\n",
    "    loaded_cvec = dill.load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_prediction(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    bow = loaded_cvec.transform(pd.Series([cleaned_text]))\n",
    "    prediction = model.predict(bow)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prediction('ไม่เป็นไร เธอทำดีแล้ว')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d08cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
